# ğŸš€ WEAVIABLE - Sistema RAG con Weaviate y Ollama

## ğŸ“ Estructura del Proyecto

```
Weaviable/
â”‚
â”œâ”€â”€ ğŸ“„ clusteConection.py          # ConexiÃ³n y configuraciÃ³n de Weaviate
â”œâ”€â”€ ğŸ“„ main.py                     # Script principal de ejecuciÃ³n
â”œâ”€â”€ ğŸ“„ pdfs.py                     # Procesamiento de PDFs
â”œâ”€â”€ ğŸ“„ subir_chunks.py             # Subida de datos con embeddings
â”œâ”€â”€ ğŸ“„ embeddings.py               # GeneraciÃ³n de embeddings con Ollama
â”œâ”€â”€ ğŸ“„ rag_query.py                # Consultas RAG con bÃºsqueda vectorial
â”œâ”€â”€ ğŸ“„ chat_interactivo.py         # ğŸŒŸ Interfaz interactiva principal
â”‚
â”œâ”€â”€ ğŸ³ docker-compose.yml          # ConfiguraciÃ³n de Weaviate local
â”œâ”€â”€ ğŸ“– viajes_demo.pdf             # Archivo PDF de ejemplo
â”‚
â”œâ”€â”€ ğŸ”§ Scripts de utilidad:
â”‚   â”œâ”€â”€ diagnostico.py             # DiagnÃ³stico del sistema
â”‚   â”œâ”€â”€ test_query.py              # Pruebas de consultas
â”‚   â”œâ”€â”€ test_search.py             # Pruebas de bÃºsqueda
â”‚   â”œâ”€â”€ limpiar_datos.py           # Limpieza de datos
â”‚   â”œâ”€â”€ probar_embeddings.py       # Pruebas de embeddings
â”‚   â”œâ”€â”€ consultas_demo.py          # Demo de consultas mÃºltiples
â”‚   â””â”€â”€ verificar_campos.py        # VerificaciÃ³n de campos disponibles
â”‚
â””â”€â”€ ğŸ“š DocumentaciÃ³n:
    â””â”€â”€ ESTRUCTURA_PROYECTO.md     # Este archivo
```

## ğŸ—ï¸ Arquitectura del Sistema

### ğŸ”„ Flujo de Datos
```
PDF Input â†’ Text Extraction â†’ Text Chunking â†’ Embedding Generation â†’ Weaviate Storage
    â†“
Query Input â†’ Query Embedding â†’ Vector Search â†’ Context Retrieval â†’ Ollama Response
```

### ğŸ§© Componentes Principales

#### 1. **ğŸ”Œ ConexiÃ³n (clusteConection.py)**
- Conecta a Weaviate local (Docker)
- Crea esquema de datos
- Valida conexiÃ³n

#### 2. **ğŸ“– Procesamiento (pdfs.py)**
- Extrae texto de PDFs
- Divide en chunks manejables
- Prepara datos para vectorizaciÃ³n

#### 3. **ğŸ§  Embeddings (embeddings.py)**
- Genera vectores usando Ollama
- Modelo: llama3.1 (4096 dimensiones)
- API local: http://localhost:11434

#### 4. **ğŸ’¾ Almacenamiento (subir_chunks.py)**
- Inserta datos con vectores
- Metadatos estructurados
- Progreso visual

#### 5. **ğŸ” Consultas (rag_query.py)**
- BÃºsqueda vectorial (near_vector)
- BÃºsqueda hÃ­brida (BM25 + vectorial)
- Retrieval de contexto relevante

#### 6. **ğŸ’¬ Interfaz (chat_interactivo.py)**
- Chat interactivo en terminal
- BÃºsqueda inteligente
- Respuestas de Ollama

## âš™ï¸ ConfiguraciÃ³n TÃ©cnica

### ğŸ³ Docker (Weaviate Local)
```yaml
# docker-compose.yml
services:
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.25.7
    ports: ["8080:8080", "50051:50051"]
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
      TEXT2VEC_OLLAMA_API_ENDPOINT: 'http://host.docker.internal:11434'
```

### ğŸ¤– Modelos de Ollama
- **llama3.1** (4.9GB) - Embeddings y generaciÃ³n
- **gemma3:1b** (815MB) - Alternativa ligera

### ğŸ“Š Esquema de Datos
```python
{
    "destino": DataType.TEXT,
    "tipo_viaje": DataType.TEXT,
    "transporte": DataType.TEXT,
    "rating": DataType.NUMBER,
    "duracion": DataType.TEXT,
    "descripcion": DataType.TEXT,
    "chunk_index": DataType.NUMBER,
    "total_chunks": DataType.NUMBER
}
```

## ğŸš€ Comandos de Uso

### Inicializar Sistema
```bash
# 1. Levantar Weaviate
docker-compose up -d

# 2. Procesar y subir datos
python main.py

# 3. Usar chat interactivo
python chat_interactivo.py
```

### Utilidades
```bash
# DiagnÃ³stico completo
python diagnostico.py

# Limpiar datos
python limpiar_datos.py

# Probar consultas
python test_search.py
```

## ğŸ¯ Capacidades del Sistema

### âœ… **Funcionalidades Implementadas**
- ğŸ“– Procesamiento de PDFs
- ğŸ§  GeneraciÃ³n de embeddings con Ollama
- ğŸ’¾ Almacenamiento vectorial en Weaviate
- ğŸ” BÃºsqueda hÃ­brida (vectorial + texto)
- ğŸ’¬ Interfaz interactiva
- ğŸ¤– Respuestas inteligentes con LLM

### ğŸ“Š **Tipos de Consultas Soportadas**
- BÃºsqueda por similitud semÃ¡ntica
- Filtrado por rating, destino, tipo
- Comparaciones entre opciones
- Recomendaciones personalizadas

### ğŸ› ï¸ **Herramientas de DiagnÃ³stico**
- VerificaciÃ³n de conexiones
- AnÃ¡lisis de datos almacenados
- Pruebas de embeddings
- Monitoreo de rendimiento

## ğŸ”§ Dependencias

### Python Packages
```
weaviate-client==4.16.10
pypdf
requests
```

### Servicios Externos
- Docker Desktop
- Ollama (local)
- Weaviate (Docker container)

## ğŸ“ˆ MÃ©tricas del Sistema
- **Documentos procesados**: 4 chunks
- **DimensiÃ³n de embeddings**: 4096
- **Modelos disponibles**: 2 (llama3.1, gemma3:1b)
- **Tiempo de respuesta**: ~10-30 segundos

## ğŸ‰ Estado Actual
âœ… **Sistema completamente funcional**  
âœ… **Weaviate local operativo**  
âœ… **Ollama integrado**  
âœ… **Embeddings funcionando**  
âœ… **Chat interactivo listo**  
âœ… **BÃºsqueda hÃ­brida implementada**

---
*Proyecto creado: 25 de septiembre, 2025*  
*TecnologÃ­as: Python, Weaviate, Ollama, Docker*